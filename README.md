The goal of this project is two-fold: to automate a real-life drudge task currently performed manually at my workplace, and, at the same time, to test the capabilities of the current crop of AI tools to generate and complete (almost) end-to-end a relatively small but useful software project, dealing with realistic challenges, unlike most toy projects used in demos (the ubiquitous "generate a web site", or worse, "change the theme of my wbesite to dark mode", for which we've had very good productivity tools for a very long time).

I also had as a subgoal to try to evaluate the AI tools for free. If the tools prove good enough to be useful, I don't mind paying for them, but I don't want to pay for the privilege of kicking the tires.

I started off with Claude Sonnet 4.0 in interactive mode, trying to define the requirements. It took a while, and a lot of back and forth, over several sessions, but it worked, and I was happy with the result. This was expected, as LLMS are quite good at understanding and generating text. 
I asked Sonnet to draft both a foundation prompt for instructing an AI to design the system, and a separate, more detailed requirements document, as I was afraid that putting too many details upfront in the initial prompt would overwhelm the AI (Sonnet agreed with this).
You can find both documents published alongside this README. 

Armed with these two documents, I wanted to try a frontier model, considered by many to the best at designing and generating code, Claude Opus 4.1.
This model was not available in a free tier, so I had to pay the minimal monthly subscription of 20 USD to get to use it. I gave it the foundation prompt for the overall design, and then the detailed requirements for refinement and code generation.
The code generation was quite painful, interrupted numerous times by hitting the limits of what the 20 USD plan entitles a user to. I asked Opus to generate Java code, which it almost did, except it generated all the classes in one large document. It also had problems with the size of this resulting document, with its final edits/refactorings failing and resulting in corrupt text/code.
I then asked Opus to generate separate files per classes, as is anyway the norm in Java. This still took a very long time, with multiple interruptions because of hitting the plan limits, but at least the individual files were not corrupt. They were still not correct Java, as the file names did not correspond to the classes defined within, so I had to do some manual fixups (changing the names of the files to match the classes, and creating the folder structure corresponding to the packages defined) before I could load the result as an Intellij (Gradle) project. Opus also generated a build.gradle file for the project, but this was also not fully correct, one of the library names was wrong.
Overall, not very impressed with Opus, nor with Anthropic's plans/tiers. It looks like to do anything useful, you need to spend over a hundred dollars per month, and the results are not great for realistic, enterprise kind of software.

Now, I still did not want to manually fix the generated mess, so I would have liked to try Claude Code, but it cannot be tried for free, nor within the 20 USD per month plan, and Opus had not given me enough confidence to pony up an extra more than 100 USD just to try it out.

I did find though a seemingly good solution in Google Jules. It cannot work on your local files system, it requires you to publish your code as a Github project, but that was a small price to pay. So here we are, on Github. 
The initial commit was my local Intellij project, with the minimal manual fixes described above. Other than that, it was the code as generated by Opus.
From then on, it was interactive work, back and forth with Jules. Now, Jules is free, and they explicitly say it works asynchronously, so one should not expect instantaneous responses. I could live with that, the problem is that multiple times it just went unresponsive, so after patiently waiting for a couple of hours or more, I would have to "wake it up". They claim that it would "ping" you when it's done, but that never happened.
Other than that, I was quite happy with the results. It took several iterations, but in the end, this is something that I could now take over and start testing/debugging with real data. 
I cannot tell yet if one the success criteria for the automation (to achieve the same accuracy as a "smart intern" doing the work manually) will be reached, but interacting with Jules for the software development part did feel like I was talking to a "smart intern". And with a faster tunraround.

All in all, it took me about two weekends to get the project in the current state - a bit over one weekend defining the requirements with Sonnet and then generating the code with Opus, then a bit less for working with Jules to fix the code and generate some unit tests
